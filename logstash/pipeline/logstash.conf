input {
  file {
    #path => "/Users/akshatha/Desktop/Personal_repo/CICD-ETL-Pipeline/data/E-commerce Website Logs.csv"    # Update with the actual path to your CSV file
    path => "/data/new_logs.csv"
    start_position => "beginning"
    sincedb_path => "/dev/null" # This ensures the file is reprocessed on each Logstash restart (useful for testing)
  }
}

filter {
  csv {
    separator => ","                                   # CSV file separator
    columns => ["Time", "Log comp", "Log subtype", "Username", "Firewall rule", "Firewall rule name",
                "NAT rule", "NAT rule name", "In interface", "Out interface", "Src IP", "Dst IP", "Src port",
                "Dst port", "protocol", "Rule type", "Live PCAP","Message","Log occurrence"]       # Specify columns based on the CSV structure
  }
  }

output {
  elasticsearch {
    hosts => ["http://es8:9200"]            # Replace with your Elasticsearch URL
    index => "firewall_logs"                          # The target index in Elasticsearch
  }
  stdout { codec => rubydebug }                        # Output to console for debugging
}